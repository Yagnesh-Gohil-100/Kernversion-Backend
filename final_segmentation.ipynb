{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row mapping: [(1, 71, 78), (2, 105, 116), (3, 121, 136), (4, 144, 147), (5, 154, 170), (6, 174, 177), (7, 182, 188), (8, 221, 223), (9, 255, 261), (10, 288, 320), (11, 325, 328), (12, 337, 349), (13, 356, 359), (14, 363, 369), (15, 377, 386), (16, 392, 405), (17, 416, 419), (18, 426, 441), (19, 443, 445), (20, 446, 448), (21, 449, 452), (22, 453, 459), (23, 501, 504), (24, 541, 553), (25, 581, 590), (26, 596, 598), (27, 621, 629), (28, 635, 649), (29, 655, 658), (30, 665, 680), (31, 685, 689), (32, 693, 699), (33, 706, 715), (34, 721, 737), (35, 742, 745), (36, 752, 767), (37, 774, 776), (38, 779, 786)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import fitz\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply Gaussian blur to the entire image\n",
    "    blurred = cv2.GaussianBlur(gray, (9, 9), 0)\n",
    "    \n",
    "    # Adaptive thresholding on blurred image\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 5)\n",
    "\n",
    "    # Morphological closing to connect broken parts of characters\n",
    "    closing_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, closing_kernel)\n",
    "\n",
    "    # Apply erosion to separate vertical lines\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    eroded = cv2.erode(closed, kernel, iterations=1)\n",
    "    \n",
    "    return eroded\n",
    "\n",
    "def enlarge_image(image, scale_factor=3):\n",
    "    enlarged_image = cv2.resize(image, (0, 0), fx=scale_factor, fy=scale_factor, interpolation=cv2.INTER_LANCZOS4)\n",
    "    return enlarged_image\n",
    "\n",
    "def enhance_quality(image):\n",
    "    sharpened = cv2.filter2D(image, -1, np.array([[-1, -1, -1], [-1,  9, -1], [-1, -1, -1]]))\n",
    "    denoised = cv2.fastNlMeansDenoisingColored(sharpened, None, 10, 10, 7, 21)\n",
    "    return denoised\n",
    "\n",
    "def create_mapping(coordinates, aspect_ratio_threshold, is_row=True):\n",
    "    mapping = []\n",
    "    number = 1\n",
    "\n",
    "    if not coordinates:\n",
    "        return mapping\n",
    "\n",
    "    for i, (page_num, x, y, w, h) in enumerate(coordinates):\n",
    "        if is_row:\n",
    "            if h / w > aspect_ratio_threshold:\n",
    "                continue\n",
    "            coord = y\n",
    "            size = h\n",
    "        else:\n",
    "            # if h / w > aspect_ratio_threshold:\n",
    "            #     continue\n",
    "            if w / h > aspect_ratio_threshold and w > 14 and h < 8:  \n",
    "                continue\n",
    "            coord = x\n",
    "            size = w\n",
    "\n",
    "        if i == 0:\n",
    "            upper_limit = coord + int(size / 2)\n",
    "            lower_limit = coord\n",
    "            mapping.append((number, lower_limit, upper_limit))\n",
    "        elif mapping and coord > mapping[-1][2]:\n",
    "            number += 1\n",
    "            lower_limit = coord\n",
    "            upper_limit = coord + int(size / 2)\n",
    "            mapping.append((number, lower_limit, upper_limit))\n",
    "        else:\n",
    "            upper_limit = max(mapping[-1][2], coord + int(size / 2))\n",
    "            mapping[-1] = (number, mapping[-1][1], upper_limit)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def assign_number(coord, mapping):\n",
    "    for num, lower_limit, upper_limit in mapping:\n",
    "        if lower_limit <= coord <= upper_limit:\n",
    "            return num\n",
    "    return -1\n",
    "\n",
    "def extract_alphabets(pdf_path, output_folder, aspect_ratio_threshold=3):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    coordinates = []\n",
    "\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        page_image = page.get_pixmap()\n",
    "        np_page_image = np.frombuffer(page_image.samples, dtype=np.uint8).reshape((page_image.height, page_image.width, page_image.n))\n",
    "\n",
    "        processed_image = preprocess_image(np_page_image)\n",
    "\n",
    "        contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            coordinates.append((page_num, x, y, w, h))\n",
    "\n",
    "    if not coordinates:\n",
    "        print(\"No contours found.\")\n",
    "        return\n",
    "\n",
    "    coordinates_sorted_by_y = sorted(coordinates, key=lambda item: item[2])\n",
    "\n",
    "    row_mapping = create_mapping(coordinates_sorted_by_y, aspect_ratio_threshold, is_row=True)\n",
    "\n",
    "    print(\"Row mapping:\", row_mapping)\n",
    "\n",
    "    for page_num, x, y, w, h in coordinates:\n",
    "        if h / w > aspect_ratio_threshold:\n",
    "            continue\n",
    "\n",
    "        row_num = assign_number(y, row_mapping)\n",
    "\n",
    "        if row_num == -1:\n",
    "            continue\n",
    "\n",
    "        alphabet_region = np_page_image[y:y+h, x:x+w]\n",
    "        enlarged_region = enlarge_image(alphabet_region)\n",
    "        enhanced_region = enhance_quality(enlarged_region)\n",
    "\n",
    "        if w<6 and h<6:\n",
    "            continue\n",
    "\n",
    "        base_filename = f\"{page_num}_row{row_num}_x{x}_y{y}_w{w}_h{h}\"\n",
    "        counter = 1\n",
    "        filename = f\"{base_filename}.png\"\n",
    "        while os.path.exists(os.path.join(output_folder, filename)):\n",
    "            filename = f\"{base_filename}_{counter}.png\"\n",
    "            counter += 1\n",
    "\n",
    "        alphabet_image = Image.fromarray(enhanced_region)\n",
    "        alphabet_image.save(os.path.join(output_folder, filename))\n",
    "    \n",
    "    return coordinates, row_mapping\n",
    "\n",
    "pdf_path = \"Analysis/check2.pdf\"\n",
    "output_folder = \"Analysis/check2\"\n",
    "coordinates, row_mapping = extract_alphabets(pdf_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First valid row number: 4\n"
     ]
    }
   ],
   "source": [
    "def find_general_boundaries(coordinates):\n",
    "    min_x = min(coordinates, key=lambda item: item[1])[1]\n",
    "    max_x = max(coordinates, key=lambda item: item[1] + item[3])[1] + max(coordinates, key=lambda item: item[1] + item[3])[3]\n",
    "    return min_x, max_x\n",
    "\n",
    "def is_row_centered(row_coords, general_min_x, general_max_x, threshold=0.15):\n",
    "    min_x = min(row_coords, key=lambda item: item[0])[0]\n",
    "    max_x = max(row_coords, key=lambda item: item[0] + item[2])[0] + max(row_coords, key=lambda item: item[0] + item[2])[2]\n",
    "    \n",
    "    center_region_left = general_min_x + (general_max_x - general_min_x) * threshold\n",
    "    center_region_right = general_max_x - (general_max_x - general_min_x) * threshold\n",
    "    \n",
    "    return center_region_left <= min_x and max_x <= center_region_right\n",
    "\n",
    "def find_first_valid_row(coordinates, row_mapping):\n",
    "    general_min_x, general_max_x = find_general_boundaries(coordinates)\n",
    "    first_valid_row = None\n",
    "    \n",
    "    for i, (row_num, lower_limit, upper_limit) in enumerate(row_mapping):\n",
    "        row_coords = [(x, y, w, h) for page_num, x, y, w, h in coordinates if lower_limit <= y <= upper_limit]\n",
    "        \n",
    "        if len(row_coords) >= 1:  # Ensure there is at least one image in the row\n",
    "            if not is_row_centered(row_coords, general_min_x, general_max_x):\n",
    "                if first_valid_row is None:\n",
    "                    first_valid_row = row_num\n",
    "                    # Check the previous row only once after finding the first valid row\n",
    "                    if i > 0:\n",
    "                        prev_row_num, prev_lower_limit, prev_upper_limit = row_mapping[i-1]\n",
    "                        prev_row_coords = [(x, y, w, h) for page_num, x, y, w, h in coordinates if prev_lower_limit <= y <= prev_upper_limit]\n",
    "\n",
    "                        if len(prev_row_coords) > 2:\n",
    "                            return prev_row_num\n",
    "                        elif len(prev_row_coords) <= 2:\n",
    "                            valid_prev_row = False\n",
    "                            for (x, y, w, h) in prev_row_coords:\n",
    "                                if ((w / h > 1.6) and h > 8) or (w < 5):\n",
    "                                    valid_prev_row = True\n",
    "                                    break\n",
    "                            if not valid_prev_row:\n",
    "                                return prev_row_num\n",
    "                    return first_valid_row\n",
    "                else:\n",
    "                    return first_valid_row\n",
    "    return None  # In case no valid row is found\n",
    "\n",
    "# Find and store the first valid row number\n",
    "first_valid_row = find_first_valid_row(coordinates, row_mapping)\n",
    "print(\"First valid row number:\", first_valid_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Load the trained model\n",
    "# model = load_model('../CNN_Model/cnn_recognizer_music_13_v2.h5')\n",
    "\n",
    "# # Preprocess the input image\n",
    "# def preprocess_image(image_path):\n",
    "#     image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     image = cv2.resize(image, (32, 32))  # Resize to match the model's input size\n",
    "#     image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "#     return image\n",
    "\n",
    "# # Pass the image through the model and get predictions\n",
    "# def predict_class(image_path):\n",
    "#     preprocessed_image = preprocess_image(image_path)\n",
    "#     predictions = model.predict(preprocessed_image)\n",
    "#     predicted_class_index = np.argmax(predictions, axis=1)\n",
    "#     max_probability = np.max(predictions, axis=1)\n",
    "#     return predicted_class_index[0], max_probability[0]\n",
    "\n",
    "# # Function to find important rows based on image filenames and class labels\n",
    "# def find_important_rows(output_folder, first_valid_row, primary_label, secondary_labels):\n",
    "#     important_rows = set()\n",
    "\n",
    "#     for filename in os.listdir(output_folder):\n",
    "#         if filename.endswith(\".png\"):\n",
    "#             # Parse filename to get row number\n",
    "#             parts = filename.split(\"_\")\n",
    "#             row_num = int(parts[1].replace(\"row\", \"\"))\n",
    "\n",
    "#             # Check if this row is at least the first valid row\n",
    "#             if row_num < first_valid_row:\n",
    "#                 continue\n",
    "\n",
    "#             # Predict class labels for all images in the current row\n",
    "#             row_images = [f for f in os.listdir(output_folder) if f.startswith(f\"{parts[0]}_row{row_num}_\")]\n",
    "#             row_predictions = [predict_class(os.path.join(output_folder, img))[0] for img in row_images]\n",
    "\n",
    "#             # Check if the row contains the primary label and meets the secondary conditions\n",
    "#             if primary_label in row_predictions:\n",
    "#                 if len(row_predictions) == 2:\n",
    "#                     if all(label in secondary_labels for label in row_predictions if label != primary_label):\n",
    "#                         important_rows.add(row_num)\n",
    "#                 else:\n",
    "#                     secondary_count = sum(1 for label in row_predictions if label in secondary_labels)\n",
    "#                     if secondary_count >= 1 and secondary_count == len(set(row_predictions) & secondary_labels):\n",
    "#                         important_rows.add(row_num)\n",
    "\n",
    "#     return sorted(important_rows)\n",
    "\n",
    "# # Assuming output_folder, first_valid_row, and important_labels are defined\n",
    "# # output_folder = \"Analysis/alhaiya_bilawal_3_taal\"\n",
    "# primary_label = 35  # Primary label index\n",
    "# secondary_labels = {31, 32, 36, 37}  # Secondary label indices\n",
    "\n",
    "# important_rows = find_important_rows(output_folder, first_valid_row, primary_label, secondary_labels)\n",
    "# print(\"Important rows:\", important_rows)\n",
    "\n",
    "\n",
    "#  ------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "important_rows = [7, 10, 12, 15, 20, 24, 28, 32]  # yaman_1_taal\n",
    "\n",
    "# important_rows = [9, 15, 20, 25, 28, 34] # bilawal_dhamaar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup range: [1, 7]\n",
      "First valid row in subgroup:  4\n",
      "Subgroup range: [8, 10]\n",
      "First valid row in subgroup:  8\n",
      "Subgroup range: [11, 12]\n",
      "First valid row in subgroup:  11\n",
      "Subgroup range: [13, 15]\n",
      "First valid row in subgroup:  13\n",
      "Subgroup range: [16, 20]\n",
      "First valid row in subgroup:  17\n",
      "Subgroup range: [21, 24]\n",
      "First valid row in subgroup:  21\n",
      "Subgroup range: [25, 28]\n",
      "First valid row in subgroup:  25\n",
      "Subgroup range: [29, 32]\n",
      "First valid row in subgroup:  29\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_details(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        x = int(match.group(3))\n",
    "        y = int(match.group(4))\n",
    "        w = int(match.group(5))\n",
    "        h = int(match.group(6))\n",
    "        return (page_num, row_num, x, y, w, h)\n",
    "    return None\n",
    "\n",
    "def update_image_filename(output_folder, old_filename, new_suffix):\n",
    "    details = get_image_details(old_filename)\n",
    "    if details:\n",
    "        page_num, row_num, x, y, w, h = details\n",
    "        new_filename = f\"{page_num}_row{row_num}_{new_suffix}_x{x}_y{y}_w{w}_h{h}.png\"\n",
    "        os.rename(os.path.join(output_folder, old_filename), os.path.join(output_folder, new_filename))\n",
    "\n",
    "def find_general_boundaries(coordinates):\n",
    "    min_x = min(coordinates, key=lambda item: item[3])[3]\n",
    "    max_x = max(coordinates, key=lambda item: item[3] + item[5])[3] + max(coordinates, key=lambda item: item[3] + item[5])[5]\n",
    "    return min_x, max_x\n",
    "\n",
    "def is_row_centered(row_coords, general_min_x, general_max_x, threshold=0.15):\n",
    "    min_x = min(row_coords, key=lambda item: item[0])[0]\n",
    "    max_x = max(row_coords, key=lambda item: item[0] + item[2])[0] + max(row_coords, key=lambda item: item[0] + item[2])[2]\n",
    "    \n",
    "    center_region_left = general_min_x + (general_max_x - general_min_x) * threshold\n",
    "    center_region_right = general_max_x - (general_max_x - general_min_x) * threshold\n",
    "    \n",
    "    return center_region_left <= min_x and max_x <= center_region_right\n",
    "\n",
    "def find_first_valid_row(coordinates, row_mapping, subgroup_lower_bound):\n",
    "    general_min_x, general_max_x = find_general_boundaries(coordinates)\n",
    "    first_valid_row = None\n",
    "    \n",
    "    for i, (row_num, lower_limit, upper_limit) in enumerate(row_mapping):\n",
    "        if row_num < subgroup_lower_bound:\n",
    "            continue\n",
    "        \n",
    "        row_coords = [(x, y, w, h) for _, _, _, x, y, w, h in coordinates if lower_limit <= y <= upper_limit]\n",
    "        \n",
    "        if len(row_coords) >= 1:  # Ensure there is at least one image in the row\n",
    "            if not is_row_centered(row_coords, general_min_x, general_max_x):\n",
    "                if first_valid_row is None:\n",
    "                    first_valid_row = row_num\n",
    "                    # Check the previous row only if the first valid row is not the lower bound of the subgroup\n",
    "                    \n",
    "                    if row_num > subgroup_lower_bound:\n",
    "                        if i > 0:\n",
    "                            prev_row_num, prev_lower_limit, prev_upper_limit = row_mapping[i-1]\n",
    "                            prev_row_coords = [(x, y, w, h) for _, _, _, x, y, w, h in coordinates if prev_lower_limit <= y <= prev_upper_limit]\n",
    "\n",
    "                            if len(prev_row_coords) > 2:\n",
    "                                return prev_row_num\n",
    "                            elif len(prev_row_coords) <= 2:\n",
    "                                valid_prev_row = False\n",
    "                                for (x, y, w, h) in prev_row_coords:\n",
    "                                    if ((w / h > 1.6) and h > 8) or (w < 5):\n",
    "                                        valid_prev_row = True\n",
    "                                        break\n",
    "                                if not valid_prev_row:\n",
    "                                    return prev_row_num\n",
    "                    return first_valid_row\n",
    "                else:\n",
    "                    return first_valid_row\n",
    "    return None  # In case no valid row is found\n",
    "\n",
    "def assign_column_numbers(output_folder, row_mapping, first_valid_row, important_rows, aspect_ratio_threshold=1.6):\n",
    "    images = os.listdir(output_folder)\n",
    "    coordinates = []\n",
    "    subgroup_ranges = []  # Store subgroup ranges\n",
    "\n",
    "    for image in images:\n",
    "        details = get_image_details(image)\n",
    "        if details:\n",
    "            page_num, row_num, x, y, w, h = details\n",
    "            if first_valid_row <= row_num <= important_rows[-1]:\n",
    "                coordinates.append((image, page_num, row_num, x, y, w, h))\n",
    "\n",
    "    important_rows = [first_valid_row - 1] + important_rows\n",
    "\n",
    "    for i in range(len(important_rows) - 1):\n",
    "        start_row = important_rows[i] + 1\n",
    "        end_row = important_rows[i + 1]\n",
    "        subgroup_coords = [\n",
    "            (image, page_num, row_num, x, y, w, h) for image, page_num, row_num, x, y, w, h in coordinates\n",
    "            if start_row <= row_num <= end_row\n",
    "        ]\n",
    "        if not subgroup_coords:\n",
    "            continue\n",
    "        \n",
    "        # Print the current subgroup range\n",
    "        print(f\"Subgroup range: [{start_row}, {end_row}]\")\n",
    "        \n",
    "        # Find and mark invalid rows in the subgroup\n",
    "        invalid_rows = []\n",
    "        first_valid_row_in_subgroup = find_first_valid_row(subgroup_coords, row_mapping, start_row)\n",
    "\n",
    "        # Print the first valid row in the subgroup\n",
    "        print(\"First valid row in subgroup: \", first_valid_row_in_subgroup)\n",
    "\n",
    "        # Store the current subgroup range as a tuple\n",
    "        subgroup_ranges.append((first_valid_row_in_subgroup, end_row-1))\n",
    "        \n",
    "        for image, page_num, row_num, x, y, w, h in subgroup_coords:\n",
    "            if row_num < first_valid_row_in_subgroup:\n",
    "                invalid_rows.append(image)\n",
    "                update_image_filename(output_folder, image, \"extra\")\n",
    "        \n",
    "        # Filter out invalid rows\n",
    "        valid_subgroup_coords = [\n",
    "            (image, page_num, row_num, x, y, w, h) for image, page_num, row_num, x, y, w, h in subgroup_coords\n",
    "            if row_num >= first_valid_row_in_subgroup\n",
    "        ]\n",
    "        \n",
    "        if not valid_subgroup_coords:\n",
    "            continue\n",
    "        \n",
    "        valid_subgroup_coords_sorted_by_x = sorted(valid_subgroup_coords, key=lambda item: item[3])  # Sort by x\n",
    "\n",
    "        valid_subgroup_coords_mapping = [\n",
    "            (page_num, x, y, w, h) for _, page_num, row_num, x, y, w, h in valid_subgroup_coords_sorted_by_x\n",
    "        ]\n",
    "\n",
    "        column_mapping = create_mapping(valid_subgroup_coords_mapping, aspect_ratio_threshold, is_row=False)\n",
    "\n",
    "        for image, page_num, row_num, x, y, w, h in valid_subgroup_coords_sorted_by_x:\n",
    "            col_num = assign_number(x, column_mapping)\n",
    "            if col_num != -1:\n",
    "                update_image_filename(output_folder, image, f\"col{col_num}\")\n",
    "        \n",
    "    return subgroup_ranges\n",
    "\n",
    "first_valid_row = 1\n",
    "subgroup_ranges = assign_column_numbers(output_folder, row_mapping, first_valid_row, important_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Subgroup range: [4, 6]\n",
      "Updated Subgroup range: [8, 9]\n",
      "Updated Subgroup range: [11, 11]\n",
      "Updated Subgroup range: [13, 14]\n",
      "Updated Subgroup range: [17, 19]\n",
      "Updated Subgroup range: [21, 23]\n",
      "Updated Subgroup range: [25, 27]\n",
      "Updated Subgroup range: [29, 31]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_image_details(filename):\n",
    "    \"\"\"\n",
    "    Extract row and column details from image filename.\n",
    "    Filename format: '0_row4_col12_x400_y145_w7_h10' or '0_row3_extra_x282_y116_w40_h18'\n",
    "    Returns (row_num, col_num).\n",
    "    \"\"\"\n",
    "    parts = filename.split('_')\n",
    "    row_num = None\n",
    "    col_num = None\n",
    "\n",
    "    for part in parts:\n",
    "        if part.startswith('row'):\n",
    "            row_num = int(part[3:])\n",
    "        elif part.startswith('col'):\n",
    "            col_num = int(part[3:])  # Ensure col_num gets a value only if it exists\n",
    "    \n",
    "    return row_num, col_num\n",
    "\n",
    "def process_subgroups(folder, subgroups):\n",
    "    \"\"\"\n",
    "    Processes subgroups by checking if the first subgroup needs to be split into two.\n",
    "    \"\"\"\n",
    "    images = os.listdir(folder)\n",
    "    images.sort(key=lambda x: get_image_details(x)[0])  # Sort by row number\n",
    "    \n",
    "    first_subgroup_start, first_subgroup_end = subgroups[0]\n",
    "    first_valid_row = first_subgroup_start\n",
    "\n",
    "    # Only process rows from the first valid row\n",
    "    first_group_images = [img for img in images if get_image_details(img)[0] >= first_valid_row]\n",
    "\n",
    "    # To track if we need to split the first subgroup\n",
    "    first_row_images = [img for img in first_group_images if get_image_details(img)[0] == first_valid_row]\n",
    "    second_row_images = [img for img in first_group_images if get_image_details(img)[0] == first_valid_row + 1]\n",
    "\n",
    "    # Ensure we have valid rows and columns to process\n",
    "    if first_row_images and second_row_images:\n",
    "        # Sort images by column number and check the first (lowest column number) image\n",
    "        first_row_images.sort(key=lambda x: get_image_details(x)[1])\n",
    "        second_row_images.sort(key=lambda x: get_image_details(x)[1])\n",
    "        \n",
    "        first_row_col = get_image_details(first_row_images[0])[1]\n",
    "        second_row_col = get_image_details(second_row_images[0])[1]\n",
    "\n",
    "        if first_row_col is not None and second_row_col is not None and first_row_col > 1 and second_row_col > 1:\n",
    "            # Now, let's iterate through rows to find where col = 1 begins\n",
    "            new_first_end = first_valid_row  # Default in case we find no rows with col = 1\n",
    "            for img in first_group_images:\n",
    "                row, col = get_image_details(img)\n",
    "                if row > first_valid_row and col == 1:\n",
    "                    new_first_end = row\n",
    "                    break\n",
    "\n",
    "            # Update the subgroups\n",
    "            first_subgroup = (first_valid_row, new_first_end)\n",
    "            second_subgroup = (new_first_end, first_subgroup_end)\n",
    "            subgroups[0] = first_subgroup\n",
    "            subgroups.insert(1, second_subgroup)\n",
    "    \n",
    "    return subgroups\n",
    "\n",
    "# Example use\n",
    "# subgroups = [(1, 9), (10, 17), (18, 24), (25, 30), (31, 35)]\n",
    "updated_subgroups = process_subgroups(\"Analysis/yaman_1_taal\", subgroup_ranges)\n",
    "\n",
    "for start, end in updated_subgroups:\n",
    "    print(f\"Updated Subgroup range: [{start}, {end}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articulation Rows:  []\n",
      "Kann Swar Rows:  [4, 13, 17, 21, 25, 29]\n",
      "Swar Rows:  [5, 8, 11, 14, 18, 22, 26, 30]\n",
      "Lyrics Rows:  [6, 9, 19, 23, 27, 31]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_image_details(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)_col(\\d+)_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3))\n",
    "        x = int(match.group(4)) \n",
    "        y = int(match.group(5))\n",
    "        w = int(match.group(6))\n",
    "        h = int(match.group(7))\n",
    "        return (page_num, row_num, col_num, x, y, w, h)\n",
    "    return None\n",
    "\n",
    "def is_articulation(w, h):\n",
    "    return 4 < h < 9 and w > 9\n",
    "\n",
    "def classify_rows_in_subgroup(subgroup_coords):\n",
    "    articulation_rows = []\n",
    "    kann_swar_rows = []\n",
    "    swar_rows = []\n",
    "    lyrics_rows = []\n",
    "\n",
    "    # Group images by rows\n",
    "    row_groups = defaultdict(list)\n",
    "    for image, page_num, row_num, x, y, w, h in subgroup_coords:\n",
    "        row_groups[row_num].append((image, page_num, x, y, w, h))\n",
    "\n",
    "    # Check for articulation rows\n",
    "    non_articulation_rows = []\n",
    "    for row_num, images in row_groups.items():\n",
    "        if all(is_articulation(w, h) for _, _, x, y, w, h in images):\n",
    "            articulation_rows.append(row_num)\n",
    "        else:\n",
    "            non_articulation_rows.append((row_num, images))\n",
    "\n",
    "    # Sort non-articulation rows by row number\n",
    "    non_articulation_rows.sort(key=lambda item: item[0])\n",
    "    remaining_rows = len(non_articulation_rows)\n",
    "\n",
    "    # Classify remaining rows based on cases\n",
    "    if remaining_rows == 3:\n",
    "        kann_swar_rows.append(non_articulation_rows[0][0])\n",
    "        swar_rows.append(non_articulation_rows[1][0])\n",
    "        lyrics_rows.append(non_articulation_rows[2][0])\n",
    "\n",
    "    elif remaining_rows == 2:\n",
    "        row1_images = non_articulation_rows[0][1]\n",
    "        row2_images = non_articulation_rows[1][1]\n",
    "        if (abs(len(row1_images) - len(row2_images)) <= 2) or (len(row1_images) > len(row2_images)):\n",
    "            swar_rows.append(non_articulation_rows[0][0])\n",
    "            lyrics_rows.append(non_articulation_rows[1][0])\n",
    "        else:\n",
    "            kann_swar_rows.append(non_articulation_rows[0][0])\n",
    "            swar_rows.append(non_articulation_rows[1][0])\n",
    "\n",
    "    elif remaining_rows == 1:\n",
    "        swar_rows.append(non_articulation_rows[0][0])\n",
    "\n",
    "    return articulation_rows, kann_swar_rows, swar_rows, lyrics_rows\n",
    "\n",
    "def process_subgroups(output_folder, subgroup_ranges, aspect_ratio_threshold=1.6):\n",
    "    images = os.listdir(output_folder)\n",
    "    coordinates = []\n",
    "\n",
    "    # Parse image details and store them\n",
    "    for image in images:\n",
    "        details = get_image_details(image)\n",
    "        if details:\n",
    "            page_num, row_num, col_num, x, y, w, h = details\n",
    "            coordinates.append((image, page_num, row_num, x, y, w, h))\n",
    "\n",
    "    articulation_rows_all = []\n",
    "    kann_swar_rows_all = []\n",
    "    swar_rows_all = []\n",
    "    lyrics_rows_all = []\n",
    "\n",
    "    # Process each subgroup range\n",
    "    for start_row, end_row in subgroup_ranges:\n",
    "        subgroup_coords = [\n",
    "            (image, page_num, row_num, x, y, w, h) for image, page_num, row_num, x, y, w, h in coordinates\n",
    "            if start_row <= row_num <= end_row\n",
    "        ]\n",
    "\n",
    "        # Classify rows within the subgroup\n",
    "        articulation_rows, kann_swar_rows, swar_rows, lyrics_rows = classify_rows_in_subgroup(subgroup_coords)\n",
    "\n",
    "        # Add rows to the respective lists\n",
    "        articulation_rows_all.extend(articulation_rows)\n",
    "        kann_swar_rows_all.extend(kann_swar_rows)\n",
    "        swar_rows_all.extend(swar_rows)\n",
    "        lyrics_rows_all.extend(lyrics_rows)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Articulation Rows: \", articulation_rows_all)\n",
    "    print(\"Kann Swar Rows: \", kann_swar_rows_all)\n",
    "    print(\"Swar Rows: \", swar_rows_all)\n",
    "    print(\"Lyrics Rows: \", lyrics_rows_all)\n",
    "\n",
    "    return articulation_rows_all, kann_swar_rows_all, swar_rows_all, lyrics_rows_all\n",
    "\n",
    "# Example usage:\n",
    "# Define the subgroup ranges as identified earlier\n",
    "# updated_subgroups = [(4, 6), (7, 8), (10, 14), (16, 19), (22, 24), (26, 27), (29, 33)]\n",
    "\n",
    "# Folder where the images are stored\n",
    "output_folder = \"Analysis/yaman_1_taal\"\n",
    "\n",
    "# Process the subgroups and classify rows\n",
    "articulation_rows, kann_swar_rows, swar_rows, lyrics_rows = process_subgroups(output_folder, updated_subgroups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bisect\n",
    "\n",
    "# def add_rows(ls):\n",
    "#     print(f\"Here is your list: {ls}\")\n",
    "#     rows = int(input(\"How many rows do you want to add?\\n\"))\n",
    "\n",
    "#     print(f\"Enter {rows} row numbers to add:\")\n",
    "#     for _ in range(rows):\n",
    "#         num = int(input())\n",
    "        \n",
    "#         # Perform binary search to find the insertion point\n",
    "#         pos = bisect.bisect_left(ls, num)\n",
    "        \n",
    "#         # Insert the number at the correct position if it doesn't already exist\n",
    "#         if pos == len(ls) or ls[pos] != num:\n",
    "#             ls.insert(pos, num)\n",
    "    \n",
    "#     print(f\"Updated list: {ls}\")\n",
    "\n",
    "\n",
    "# def delete_rows(ls):\n",
    "#     print(f\"Here is your list: {ls}\")\n",
    "#     rows = int(input(\"How many rows do you want to delete?\\n\"))\n",
    "\n",
    "#     # Using a set to store numbers to delete\n",
    "#     to_delete = set()\n",
    "\n",
    "#     print(f\"Enter {rows} row numbers to delete:\")\n",
    "#     for _ in range(rows):\n",
    "#         num = int(input())\n",
    "#         to_delete.add(num)\n",
    "\n",
    "#     # Use list comprehension to filter out the rows that need to be deleted\n",
    "#     ls[:] = [x for x in ls if x not in to_delete]\n",
    "\n",
    "#     print(f\"Updated list: {ls}\")\n",
    "\n",
    "\n",
    "# def modify_rows(articulation_rows, kann_swar_rows, swar_rows, lyrics_rows):\n",
    "#     while True:\n",
    "#         print(\"\\n1. Articulation\\n2. Kann Swar\\n3. Swar\\n4. Lyrics\\n5. Exit\")\n",
    "#         select = int(input(\"Which section do you want to modify? (Enter your choice (1-5))\\n\"))\n",
    "\n",
    "#         if select == 5:\n",
    "#             print(\"Modifications are not required further.\")\n",
    "#             break\n",
    "\n",
    "#         print(\"1. Add rows\\n2. Delete rows\\n3. Exit\")\n",
    "#         choice = int(input(\"Enter your choice (1-3)\\n\"))\n",
    "\n",
    "#         if choice == 3:\n",
    "#             print(\"No modifications in this section.\")\n",
    "#             continue\n",
    "\n",
    "#         if select == 1:\n",
    "#             if choice == 1:\n",
    "#                 add_rows(articulation_rows)\n",
    "#             else:\n",
    "#                 delete_rows(articulation_rows)\n",
    "#         elif select == 2:\n",
    "#             if choice == 1:\n",
    "#                 add_rows(kann_swar_rows)\n",
    "#             else:\n",
    "#                 delete_rows(kann_swar_rows)\n",
    "#         elif select == 3:\n",
    "#             if choice == 1:\n",
    "#                 add_rows(swar_rows)\n",
    "#             else:\n",
    "#                 delete_rows(swar_rows)\n",
    "#         elif select == 4:\n",
    "#             if choice == 1:\n",
    "#                 add_rows(lyrics_rows)\n",
    "#             else:\n",
    "#                 delete_rows(lyrics_rows)\n",
    "\n",
    "\n",
    "# # # Example usage:\n",
    "# # articulation_rows = [5, 8, 12]\n",
    "# # kann_swar_rows = [6]\n",
    "# # swar_rows = [7, 9]\n",
    "# # lyrics_rows = [10]\n",
    "\n",
    "# # Call the interactive row modification function\n",
    "# modify_rows(articulation_rows, kann_swar_rows, swar_rows, lyrics_rows)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# yaman_1_taal\n",
    "\n",
    "articulation_rows = []\n",
    "kann_swar_rows = [4, 13, 17, 21, 25, 29]\n",
    "swar_rows = [5, 8, 11, 14, 18, 22, 26, 30]\n",
    "lyrics_rows = [6, 9, 19, 23, 27, 31]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# # bilawal_dhamaar\n",
    "\n",
    "# articulation_rows = [12, 14, 31, 33]\n",
    "# kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "# swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "# lyrics_rows = [6, 8, 13, 19, 24, 27, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articulation Rows:  []\n",
      "Kann Swar Rows:  [4, 13, 17, 21, 25, 29]\n",
      "Swar Rows:  [5, 8, 11, 14, 18, 22, 26, 30]\n",
      "Lyrics Rows:  [6, 9, 19, 23, 27, 31]\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Articulation Rows: \", articulation_rows)\n",
    "print(\"Kann Swar Rows: \", kann_swar_rows)\n",
    "print(\"Swar Rows: \", swar_rows)\n",
    "print(\"Lyrics Rows: \", lyrics_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 6)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row4_col1_x102_y134_w8_h10.png'], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row4_col4_x212_y134_w11_h9.png'], ['Analysis\\\\yaman_1_taal\\\\0_row4_col5_x249_y133_w10_h10.png'], [], ['Analysis\\\\yaman_1_taal\\\\0_row4_col7_x322_y130_w9_h12.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row5_col1_x104_y147_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col2_x145_y147_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col3_x177_y146_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col4_x216_y147_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col5_x254_y143_w10_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col6_x295_y149_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col7_x327_y150_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col8_x364_y150_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col9_x400_y144_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col10_x436_y150_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col11_x467_y154_w8_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col12_x501_y149_w10_h13.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row6_col1_x105_y180_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col2_x146_y180_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col3_x177_y180_w11_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col4_x217_y175_w15_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col5_x251_y180_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col6_x295_y180_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col7_x327_y179_w8_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col8_x363_y179_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col9_x399_y179_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col10_x434_y179_w16_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col11_x466_y179_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col12_x500_y180_w9_h12.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (8, 9)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row8_col1_x103_y213_w10_h12.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal_segmented\\\\0_row8_col7_x321_y216_w13_h9_upper.png'], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row8_col10_x434_y212_w7_h13.png'], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row8_col1_x104_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col2_x148_y237_w9_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col3_x178_y233_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col4_x221_y227_w9_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col5_x250_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col6_x295_y232_w9_h13.png'], ['Analysis\\\\yaman_1_taal_segmented\\\\0_row8_col7_x321_y228_w13_h15_lower.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col8_x362_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col9_x400_y227_w7_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col10_x431_y229_w15_h23.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col11_x465_y227_w7_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col12_x496_y233_w16_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row9_col1_x104_y256_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col2_x151_y261_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col3_x177_y261_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col4_x224_y262_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col5_x250_y262_w11_h15.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col6_x295_y261_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col7_x326_y262_w12_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col8_x365_y261_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col9_x400_y262_w9_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col10_x434_y262_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col11_x466_y261_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col12_x499_y262_w11_h12.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (11, 11)\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row11_col1_x104_y306_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col2_x143_y307_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col3_x176_y301_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col4_x223_y301_w9_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col5_x250_y307_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col6_x294_y307_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col7_x327_y301_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col8_x364_y301_w9_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col9_x400_y307_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col10_x437_y307_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col11_x465_y307_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col12_x500_y307_w11_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (13, 14)\n",
      "Kann Swar List: [[], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row13_col7_x322_y339_w13_h14.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row14_col1_x105_y356_w14_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col2_x143_y356_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col3_x177_y355_w8_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col4_x223_y357_w8_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col5_x250_y356_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col6_x297_y355_w8_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col7_x328_y356_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col8_x370_y355_w7_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col9_x399_y356_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col10_x431_y356_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col11_x465_y360_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col12_x501_y360_w10_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (17, 19)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row17_col1_x103_y414_w10_h12.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row17_col7_x322_y418_w7_h10.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row18_col1_x108_y434_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col2_x145_y434_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col3_x181_y435_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col4_x218_y439_w11_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col5_x255_y434_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col6_x292_y435_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col7_x327_y431_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col8_x366_y440_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col9_x396_y431_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col10_x436_y431_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col11_x466_y440_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col12_x494_y431_w17_h16.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row19_col1_x108_y463_w12_h15.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col2_x147_y463_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col3_x182_y464_w14_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col4_x221_y464_w8_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col5_x255_y460_w15_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col6_x290_y460_w11_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col7_x327_y464_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col8_x367_y464_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col9_x397_y463_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col10_x435_y464_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col11_x466_y464_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col12_x499_y465_w11_h12.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (21, 23)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row21_col1_x102_y495_w14_h14.png'], [], [], [], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row21_col11_x434_y500_w41_h10.png'], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row22_col1_x108_y512_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col2_x138_y512_w17_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col3_x181_y512_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col4_x217_y522_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col5_x254_y513_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col6_x293_y512_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col7_x327_y513_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col8_x357_y514_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col10_x399_y513_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col11_x431_y513_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col12_x465_y518_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col13_x501_y518_w9_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row23_col1_x108_y545_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col2_x144_y546_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col3_x181_y546_w14_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col4_x219_y546_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col5_x253_y541_w14_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col6_x293_y546_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col7_x327_y547_w12_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col9_x366_y547_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col10_x400_y542_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col11_x428_y546_w19_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col12_x466_y547_w8_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col13_x501_y547_w9_h14.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (25, 27)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row25_col1_x114_y586_w35_h7.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row25_col7_x321_y583_w7_h9.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row26_col1_x108_y599_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col2_x143_y599_w11_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col3_x180_y599_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col4_x218_y599_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col5_x252_y599_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col6_x291_y600_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col7_x326_y596_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col8_x358_y596_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col9_x400_y600_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col10_x437_y600_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col11_x466_y600_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col12_x501_y600_w9_h13.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row27_col1_x108_y628_w13_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col2_x146_y628_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col3_x179_y628_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col4_x216_y628_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col5_x253_y629_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col6_x290_y629_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col7_x327_y629_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col8_x363_y630_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col9_x400_y630_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col10_x438_y630_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col11_x467_y629_w7_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col12_x503_y629_w8_h13.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 31)\n",
      "Kann Swar List: [[], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row29_col4_x211_y665_w10_h9.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row29_col10_x429_y666_w10_h9.png'], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row30_col1_x107_y677_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col2_x138_y678_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col3_x177_y677_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col4_x216_y678_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col5_x254_y677_w11_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col6_x291_y683_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col7_x325_y683_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col8_x363_y683_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col9_x400_y683_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col11_x439_y677_w8_h19.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col12_x466_y687_w11_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col13_x494_y683_w16_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row31_col1_x107_y710_w12_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col2_x144_y711_w10_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col3_x180_y711_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col4_x220_y711_w12_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col5_x254_y712_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col6_x289_y712_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col7_x326_y711_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col8_x364_y711_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col9_x399_y707_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col10_x428_y712_w19_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col12_x466_y712_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col13_x497_y713_w10_h12.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# storing image paths in lists for direct access\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the folder containing the images\n",
    "image_folder_path = 'Analysis/yaman_1_taal'\n",
    "\n",
    "# # Define the section-wise row numbers\n",
    "# articulation_rows = [12, 14, 31, 33]\n",
    "# kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "# swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "# lyrics_rows = [6, 8, 13, 19, 24, 27, 32]\n",
    "\n",
    "# # Define the subgroup ranges\n",
    "# subgroup_ranges = [\n",
    "#     (4, 7),\n",
    "#     (7, 8),\n",
    "#     (10, 14),\n",
    "#     (16, 19),\n",
    "#     (22, 24),\n",
    "#     (26, 27),\n",
    "#     (29, 33)\n",
    "# ]\n",
    "\n",
    "# store the final updated subgroup to use further\n",
    "subgroup_ranges = updated_subgroups\n",
    "\n",
    "# Define the beat count (size of the lists)\n",
    "beat_count = 12\n",
    "\n",
    "# Function to extract information from the image filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)(?:_col(\\d+))?_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3)) if match.group(3) else None\n",
    "        x = int(match.group(4))\n",
    "        y = int(match.group(5))\n",
    "        width = int(match.group(6))\n",
    "        height = int(match.group(7))\n",
    "        # Use os.path.join to handle path separators correctly\n",
    "        image_path = os.path.normpath(os.path.join(image_folder_path, filename))\n",
    "        return page_num, row_num, col_num, x, y, width, height, image_path\n",
    "    return None\n",
    "\n",
    "# Load all image filenames and extract their information\n",
    "image_files = os.listdir(image_folder_path)\n",
    "image_info = [extract_info_from_filename(f) for f in image_files]\n",
    "image_info = [info for info in image_info if info is not None]\n",
    "\n",
    "# Organize images by row and column\n",
    "row_col_images = defaultdict(lambda: defaultdict(list))\n",
    "for info in image_info:\n",
    "    page_num, row_num, col_num, x, y, width, height, image_path = info\n",
    "    row_col_images[row_num][col_num].append((x, y, width, height, image_path))\n",
    "\n",
    "# Function to pad lists to match the beat count\n",
    "def pad_lists(lists, size):\n",
    "    if len(lists) < size:\n",
    "        padding = [[] for _ in range(size - len(lists))]\n",
    "        return padding + lists\n",
    "    return lists\n",
    "\n",
    "def save_segment(segment, subgroup_range, col, part_type, original_filename):\n",
    "    \"\"\"\n",
    "    Function to save a segmented part and return its path.\n",
    "    \n",
    "    Parameters:\n",
    "    - segment: The segmented image (enlarged by a factor of 3).\n",
    "    - subgroup_range: The subgroup range.\n",
    "    - col: The column number.\n",
    "    - part_type: Type of segment ('upper' or 'lower').\n",
    "    - original_filename: The original filename of the image before segmentation.\n",
    "    \n",
    "    Returns:\n",
    "    - Path to the saved segment.\n",
    "    \"\"\"\n",
    "    # Extract original image details from the filename\n",
    "    pattern = r'(\\d+)_row(\\d+)_col(\\d+)_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, original_filename)\n",
    "    if not match:\n",
    "        raise ValueError(f\"Original filename {original_filename} does not match the expected pattern.\")\n",
    "    \n",
    "    page_num = match.group(1)\n",
    "    row_num = match.group(2)\n",
    "    col_num = match.group(3)\n",
    "    original_x = int(match.group(4))  # x-coordinate (pre-enlarged)\n",
    "    original_y = int(match.group(5))  # y-coordinate (pre-enlarged)\n",
    "    original_w = int(match.group(6))  # width (pre-enlarged)\n",
    "    original_h = int(match.group(7))  # height (pre-enlarged)\n",
    "    \n",
    "    # Calculate new coordinates for the segmented part (scaled down by a factor of 3)\n",
    "    if part_type == 'upper':\n",
    "        # Upper part: y remains the same, height is the separation row\n",
    "        new_x = original_x\n",
    "        new_y = original_y\n",
    "        new_w = original_w\n",
    "        new_h = segment.shape[0] // 3  # Height of the upper part (scaled down)\n",
    "    elif part_type == 'lower':\n",
    "        # Lower part: y is original_y + height of the upper part, height is adjusted\n",
    "        new_x = original_x\n",
    "        new_y = original_y + (original_h - (segment.shape[0] // 3))  # Adjust y for lower part (scaled down)\n",
    "        new_w = original_w\n",
    "        new_h = segment.shape[0] // 3  # Height of the lower part (scaled down)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid part_type. Must be 'upper' or 'lower'.\")\n",
    "    \n",
    "    # Create the new filename\n",
    "    new_filename = f\"{page_num}_row{row_num}_col{col_num}_x{new_x}_y{new_y}_w{new_w}_h{new_h}_{part_type}.png\"\n",
    "    \n",
    "    # Save the segmented image\n",
    "    output_folder = os.path.normpath('Analysis/yaman_1_taal_segmented')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    segment_path = os.path.join(output_folder, new_filename)\n",
    "    cv2.imwrite(segment_path, segment)\n",
    "    \n",
    "    return segment_path\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 5)\n",
    "    return thresh\n",
    "\n",
    "# Function to separate articulation in an image\n",
    "def check_articulation(image):\n",
    "    processed_image = preprocess_image(image)\n",
    "    contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if 10 < h < 21 and w > 25:\n",
    "            upper_part = image[:y, :]\n",
    "            if upper_part.shape[0] > 0:\n",
    "                return image, True  # Return the upper part and a flag indicating segmentation was successful\n",
    "            break\n",
    "    \n",
    "    return image, False  # Return the original image and a flag indicating no segmentation\n",
    "\n",
    "# Function to process a subgroup and create the lists of lists\n",
    "def process_subgroup(subgroup_range, is_first_subgroup):\n",
    "    start_row, end_row = subgroup_range\n",
    "    \n",
    "    # Find the swar row in this subgroup\n",
    "    swar_row = None\n",
    "    for row in swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            swar_row = row\n",
    "            break\n",
    "    \n",
    "    if not swar_row:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Find the kann swar row in this subgroup\n",
    "    kann_swar_row = None\n",
    "    for row in kann_swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            kann_swar_row = row\n",
    "            break\n",
    "    \n",
    "    # Find the articulation rows in this subgroup\n",
    "    articulation_rows_in_subgroup = [row for row in articulation_rows if start_row <= row <= end_row]\n",
    "    \n",
    "    # Find the lyrics row in this subgroup\n",
    "    lyrics_row = None\n",
    "    for row in lyrics_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            lyrics_row = row\n",
    "            break\n",
    "    \n",
    "    # Get the swar images and their column numbers\n",
    "    swar_images = row_col_images[swar_row]\n",
    "    swar_cols = sorted(swar_images.keys())\n",
    "    \n",
    "    # Get the kann swar images and their column numbers (if kann swar row exists)\n",
    "    kann_swar_images = row_col_images[kann_swar_row] if kann_swar_row else {}\n",
    "    kann_swar_cols = sorted(kann_swar_images.keys())\n",
    "    \n",
    "    # Get the lyrics images (if lyrics row exists)\n",
    "    lyrics_images = row_col_images[lyrics_row] if lyrics_row else {}\n",
    "    lyrics_cols = sorted(lyrics_images.keys()) if lyrics_row else []\n",
    "    \n",
    "    # Create the lists of lists\n",
    "    swar_list = []\n",
    "    kann_swar_list = []\n",
    "    swar_articulation_checks = [False] * len(swar_cols)\n",
    "    lyrics_articulation_checks = [False] * len(lyrics_cols)\n",
    "    lyrics_list = []\n",
    "    \n",
    "    # Case 1: If there is an explicit kann swar row\n",
    "    if kann_swar_row:\n",
    "        swar_index = 0\n",
    "        kann_swar_index = 0\n",
    "        \n",
    "        while swar_index < len(swar_cols) or kann_swar_index < len(kann_swar_cols):\n",
    "            swar_col = swar_cols[swar_index] if swar_index < len(swar_cols) else None\n",
    "            kann_swar_col = kann_swar_cols[kann_swar_index] if kann_swar_index < len(kann_swar_cols) else None\n",
    "            \n",
    "            # If both columns exist and match\n",
    "            if swar_col is not None and kann_swar_col is not None and swar_col == kann_swar_col:\n",
    "                swar_list.append([x[4] for x in swar_images[swar_col]])  # Store image paths\n",
    "                kann_swar_list.append([x[4] for x in kann_swar_images[kann_swar_col]])  # Store image paths\n",
    "                swar_index += 1\n",
    "                kann_swar_index += 1\n",
    "            # If swar column exists but kann swar column doesn't match or is missing\n",
    "            elif swar_col is not None and (kann_swar_col is None or swar_col < kann_swar_col):\n",
    "                swar_list.append([x[4] for x in swar_images[swar_col]])  # Store image paths\n",
    "                kann_swar_list.append([])\n",
    "                swar_index += 1\n",
    "            # If kann swar column exists but swar column doesn't match or is missing\n",
    "            elif kann_swar_col is not None and (swar_col is None or kann_swar_col < swar_col):\n",
    "                # Assign the kann swar to the next available swar column\n",
    "                if swar_index < len(swar_cols):\n",
    "                    swar_list.append([x[4] for x in swar_images[swar_cols[swar_index]]])  # Store image paths\n",
    "                    kann_swar_list.append([x[4] for x in kann_swar_images[kann_swar_col]])  # Store image paths\n",
    "                    swar_index += 1\n",
    "                    kann_swar_index += 1\n",
    "                else:\n",
    "                    # If no more swar columns are available, append an empty list\n",
    "                    swar_list.append([])\n",
    "                    kann_swar_list.append([x[4] for x in kann_swar_images[kann_swar_col]])  # Store image paths\n",
    "                    kann_swar_index += 1\n",
    "    \n",
    "    # Case 2: If there is no explicit kann swar row, check for hidden kann swars in the swar row\n",
    "    else:\n",
    "        for col in swar_cols:\n",
    "            images_in_col = swar_images[col]\n",
    "            \n",
    "            # Separate outlier and non-outlier images\n",
    "            outlier_images = [img for img in images_in_col if img[3] > 25]  # Images with h > 25\n",
    "            non_outlier_images = [img for img in images_in_col if img[3] <= 25]  # Images with h <= 25\n",
    "            \n",
    "            # Process outlier images\n",
    "            for img in outlier_images:\n",
    "                x, y, w, h, image_path = img  # Extract image details\n",
    "                \n",
    "                # Load the image\n",
    "                outlier_image = cv2.imread(image_path)\n",
    "                \n",
    "                # Check if articulation separation is supposed to give True\n",
    "                _, is_articulated = check_articulation(outlier_image)\n",
    "                \n",
    "                if not is_articulated:\n",
    "\n",
    "                    # Convert to grayscale\n",
    "                    gray = cv2.cvtColor(outlier_image, cv2.COLOR_BGR2GRAY)\n",
    "                    \n",
    "                    # Apply Gaussian blur to the entire image\n",
    "                    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "                    \n",
    "                    # Adaptive thresholding on blurred image\n",
    "                    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 5)\n",
    "\n",
    "                    # Morphological closing to connect broken parts of characters\n",
    "                    closing_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "                    closed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, closing_kernel)\n",
    "                    \n",
    "                    # Compute vertical projection (sum of non-white pixels for each row)\n",
    "                    vertical_projection = np.sum(closed, axis=1) / 255  # Normalize to count non-white pixels\n",
    "                    \n",
    "                    # Find the row with the minimum non-white pixels (separation line)\n",
    "                    # Define a reasonable range for the split line (e.g., 30% to 70% of the image height)\n",
    "                    height = outlier_image.shape[0]\n",
    "                    lower_bound = int(height * 0.3)  # 30% of the height\n",
    "                    upper_bound = int(height * 0.5)  # 50% of the height\n",
    "\n",
    "                    # Find the row with the minimum projection within the defined range\n",
    "                    valid_range = vertical_projection[lower_bound:upper_bound]\n",
    "                    if valid_range.size > 0:\n",
    "                        separation_row_in_range = np.argmin(valid_range)  # Find the minimum within the range\n",
    "                        separation_row = lower_bound + separation_row_in_range  # Adjust to the full image coordinates\n",
    "                    else:\n",
    "                        # If no valid split line is found, default to the middle of the image\n",
    "                        separation_row = height // 2\n",
    "                    \n",
    "                    # Split the image into upper and lower parts\n",
    "                    upper_part = outlier_image[:separation_row, :]\n",
    "                    lower_part = outlier_image[separation_row:, :]\n",
    "                    \n",
    "                    # Crop extra white background from upper part\n",
    "                    upper_gray = cv2.cvtColor(upper_part, cv2.COLOR_BGR2GRAY)\n",
    "                    _, upper_binary = cv2.threshold(upper_gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "                    upper_coords = np.column_stack(np.where(upper_binary > 0))\n",
    "                    if len(upper_coords) > 0:\n",
    "                        y1, x1 = upper_coords.min(axis=0)\n",
    "                        y2, x2 = upper_coords.max(axis=0)\n",
    "                        upper_part_cropped = upper_part[y1:y2 + 1, x1:x2 + 1]\n",
    "                    else:\n",
    "                        upper_part_cropped = upper_part  # If no non-white pixels, keep as is\n",
    "                    \n",
    "                    # Crop extra white background from lower part\n",
    "                    lower_gray = cv2.cvtColor(lower_part, cv2.COLOR_BGR2GRAY)\n",
    "                    _, lower_binary = cv2.threshold(lower_gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "                    lower_coords = np.column_stack(np.where(lower_binary > 0))\n",
    "                    if len(lower_coords) > 0:\n",
    "                        y1, x1 = lower_coords.min(axis=0)\n",
    "                        y2, x2 = lower_coords.max(axis=0)\n",
    "                        lower_part_cropped = lower_part[y1:y2 + 1, x1:x2 + 1]\n",
    "                    else:\n",
    "                        lower_part_cropped = lower_part  # If no non-white pixels, keep as is\n",
    "                    \n",
    "                    # Save the cropped parts with updated naming convention\n",
    "                    original_filename = os.path.basename(image_path)  # Extract original filename\n",
    "                    upper_part_path = save_segment(upper_part_cropped, subgroup_range, col, 'upper', original_filename)\n",
    "                    lower_part_path = save_segment(lower_part_cropped, subgroup_range, col, 'lower', original_filename)\n",
    "                    \n",
    "                    # Store the paths in the lists\n",
    "                    swar_list.append([lower_part_path])  # Lower part goes to swar_list\n",
    "                    kann_swar_list.append([upper_part_path])  # Upper part goes to kann_swar_list\n",
    "                else:\n",
    "                    # If articulation separation is supposed to give True, keep the original image\n",
    "                    swar_list.append([image_path])\n",
    "                    kann_swar_list.append([])\n",
    "            \n",
    "            # Process non-outlier images\n",
    "            if len(non_outlier_images) == 1:\n",
    "                # Only one image in this column, so no hidden kann swar\n",
    "                swar_list.append([non_outlier_images[0][4]])  # Store image path\n",
    "                kann_swar_list.append([])\n",
    "            elif len(non_outlier_images) > 1:\n",
    "                # Multiple images in the same column, so identify hidden kann swars\n",
    "                # Sort images by y-value (lower y-value is kann swar)\n",
    "                sorted_images = sorted(non_outlier_images, key=lambda x: x[1])  # Sort by y-value\n",
    "                kann_swar_list.append([sorted_images[0][4]])  # Lower y-value is kann swar (store image path)\n",
    "                swar_list.append([sorted_images[1][4]])  # Higher y-value is swar (store image path)\n",
    "    \n",
    "    # Handle articulation rows\n",
    "    for articulation_row in articulation_rows_in_subgroup:\n",
    "        # Find the row just before the articulation row\n",
    "        prev_row = articulation_row - 1\n",
    "        if prev_row in swar_rows:\n",
    "            # Swar articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    swar_articulation_checks[i] = True\n",
    "        elif prev_row in lyrics_rows:\n",
    "            # Lyrics articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    lyrics_articulation_checks[i] = True\n",
    "    \n",
    "    # Handle lyrics row (append images one by one without comparing column numbers)\n",
    "    if lyrics_row:\n",
    "        # Get all lyrics images in order\n",
    "        lyrics_cols = sorted(lyrics_images.keys())\n",
    "        for col in lyrics_cols:\n",
    "            lyrics_list.append([x[4] for x in lyrics_images[col]])  # Store image paths\n",
    "    else:\n",
    "        lyrics_list = [[] for _ in range(len(swar_cols))]\n",
    "    \n",
    "    # Pad lists to match the beat count\n",
    "    if is_first_subgroup:\n",
    "        swar_list = pad_lists(swar_list, beat_count)\n",
    "        kann_swar_list = pad_lists(kann_swar_list, beat_count)\n",
    "        swar_articulation_checks = pad_lists(swar_articulation_checks, beat_count)\n",
    "        lyrics_articulation_checks = pad_lists(lyrics_articulation_checks, beat_count)\n",
    "        lyrics_list = pad_lists(lyrics_list, beat_count)\n",
    "    else:\n",
    "        if len(swar_list) < beat_count:\n",
    "            swar_list += [[] for _ in range(beat_count - len(swar_list))]\n",
    "        if len(kann_swar_list) < beat_count:\n",
    "            kann_swar_list += [[] for _ in range(beat_count - len(kann_swar_list))]\n",
    "        if len(swar_articulation_checks) < beat_count:\n",
    "            swar_articulation_checks += [False for _ in range(beat_count - len(swar_articulation_checks))]\n",
    "        if len(lyrics_articulation_checks) < beat_count:\n",
    "            lyrics_articulation_checks += [False for _ in range(beat_count - len(lyrics_articulation_checks))]\n",
    "        if len(lyrics_list) < beat_count:\n",
    "            lyrics_list += [[] for _ in range(beat_count - len(lyrics_list))]\n",
    "    \n",
    "    return swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list\n",
    "\n",
    "# Process each subgroup and store the results\n",
    "subgroup_results = {}\n",
    "for i, subgroup_range in enumerate(subgroup_ranges):\n",
    "    is_first_subgroup = (i == 0)\n",
    "    swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list = process_subgroup(subgroup_range, is_first_subgroup)\n",
    "    if swar_list and kann_swar_list:\n",
    "        subgroup_results[subgroup_range] = {\n",
    "            'swar_list': swar_list,\n",
    "            'kann_swar_list': kann_swar_list,\n",
    "            'swar_articulation_checks': swar_articulation_checks,\n",
    "            'lyrics_articulation_checks': lyrics_articulation_checks,\n",
    "            'lyrics_list': lyrics_list\n",
    "        }\n",
    "\n",
    "# Print the results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Swar Articulation Checks: {results['swar_articulation_checks']}\")\n",
    "    print(f\"Lyrics List: {results['lyrics_list']}\")\n",
    "    print(f\"Lyrics Articulation Checks: {results['lyrics_articulation_checks']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 6)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row4_col1_x102_y134_w8_h10.png'], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row4_col4_x212_y134_w11_h9.png'], ['Analysis\\\\yaman_1_taal\\\\0_row4_col5_x249_y133_w10_h10.png'], [], ['Analysis\\\\yaman_1_taal\\\\0_row4_col7_x322_y130_w9_h12.png'], [], [], [], [], []]\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row5_col1_x104_y147_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col2_x145_y147_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col3_x177_y146_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col4_x216_y147_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col5_x254_y143_w10_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col6_x295_y149_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col7_x327_y150_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col8_x364_y150_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col9_x400_y144_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col10_x436_y150_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col11_x467_y154_w8_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col12_x501_y149_w10_h13.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row6_col1_x105_y180_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col2_x146_y180_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col3_x177_y180_w11_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col4_x217_y175_w15_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col5_x251_y180_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col6_x295_y180_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col7_x327_y179_w8_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col8_x363_y179_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col9_x399_y179_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col10_x434_y179_w16_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col11_x466_y179_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col12_x500_y180_w9_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (8, 9)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row8_col1_x103_y213_w10_h12.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal_segmented\\\\0_row8_col7_x321_y216_w13_h9_upper.png'], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row8_col10_x434_y212_w7_h13.png'], [], []]\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row8_col1_x104_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col2_x148_y237_w9_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col3_x178_y233_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col4_x221_y227_w9_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col5_x250_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col6_x295_y232_w9_h13.png'], ['Analysis\\\\yaman_1_taal_segmented\\\\0_row8_col7_x321_y228_w13_h15_lower.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col8_x362_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col9_x400_y227_w7_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col10_x431_y229_w15_h23.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col11_x465_y227_w7_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col12_x496_y233_w16_h12.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row9_col1_x104_y256_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col2_x151_y261_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col3_x177_y261_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col4_x224_y262_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col5_x250_y262_w11_h15.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col6_x295_y261_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col7_x326_y262_w12_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col8_x365_y261_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col9_x400_y262_w9_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col10_x434_y262_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col11_x466_y261_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col12_x499_y262_w11_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (11, 11)\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row11_col1_x104_y306_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col2_x143_y307_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col3_x176_y301_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col4_x223_y301_w9_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col5_x250_y307_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col6_x294_y307_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col7_x327_y301_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col8_x364_y301_w9_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col9_x400_y307_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col10_x437_y307_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col11_x465_y307_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col12_x500_y307_w11_h12.png']]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (13, 14)\n",
      "Kann Swar List: [[], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row13_col7_x322_y339_w13_h14.png'], [], [], [], [], []]\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row14_col1_x105_y356_w14_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col2_x143_y356_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col3_x177_y355_w8_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col4_x223_y357_w8_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col5_x250_y356_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col6_x297_y355_w8_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col7_x328_y356_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col8_x370_y355_w7_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col9_x399_y356_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col10_x431_y356_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col11_x465_y360_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col12_x501_y360_w10_h12.png']]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (17, 19)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row17_col1_x103_y414_w10_h12.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row17_col7_x322_y418_w7_h10.png'], [], [], [], [], []]\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row18_col1_x108_y434_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col2_x145_y434_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col3_x181_y435_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col4_x218_y439_w11_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col5_x255_y434_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col6_x292_y435_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col7_x327_y431_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col8_x366_y440_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col9_x396_y431_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col10_x436_y431_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col11_x466_y440_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col12_x494_y431_w17_h16.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row19_col1_x108_y463_w12_h15.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col2_x147_y463_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col3_x182_y464_w14_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col4_x221_y464_w8_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col5_x255_y460_w15_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col6_x290_y460_w11_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col7_x327_y464_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col8_x367_y464_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col9_x397_y463_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col10_x435_y464_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col11_x466_y464_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col12_x499_y465_w11_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (21, 23)\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row21_col1_x102_y495_w14_h14.png'], [], [], [], [], [], [], [], [], ['Analysis\\\\yaman_1_taal_segmented\\\\21_23_9_left.png'], [], []]\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', 'S', 'E', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row22_col1_x108_y512_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col2_x138_y512_w17_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col3_x181_y512_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col4_x217_y522_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col5_x254_y513_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col6_x293_y512_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col7_x327_y513_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col8_x357_y514_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col10_x399_y513_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col11_x431_y513_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col12_x465_y518_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col13_x501_y518_w9_h12.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row23_col1_x108_y545_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col2_x144_y546_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col3_x181_y546_w14_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col4_x219_y546_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col5_x253_y541_w14_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col6_x293_y546_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col7_x327_y547_w12_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col9_x366_y547_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col10_x400_y542_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col11_x428_y546_w19_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col12_x466_y547_w8_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col13_x501_y547_w9_h14.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (25, 27)\n",
      "Kann Swar List: [[], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row25_col7_x321_y583_w7_h9.png'], [], [], [], [], []]\n",
      "Meend List: ['S', 'E', '', '', '', '', '', '', '', '', '', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row26_col1_x108_y599_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col2_x143_y599_w11_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col3_x180_y599_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col4_x218_y599_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col5_x252_y599_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col6_x291_y600_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col7_x326_y596_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col8_x358_y596_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col9_x400_y600_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col10_x437_y600_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col11_x466_y600_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col12_x501_y600_w9_h13.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row27_col1_x108_y628_w13_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col2_x146_y628_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col3_x179_y628_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col4_x216_y628_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col5_x253_y629_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col6_x290_y629_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col7_x327_y629_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col8_x363_y630_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col9_x400_y630_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col10_x438_y630_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col11_x467_y629_w7_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col12_x503_y629_w8_h13.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 31)\n",
      "Kann Swar List: [[], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row29_col4_x211_y665_w10_h9.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row29_col10_x429_y666_w10_h9.png'], [], []]\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row30_col1_x107_y677_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col2_x138_y678_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col3_x177_y677_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col4_x216_y678_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col5_x254_y677_w11_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col6_x291_y683_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col7_x325_y683_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col8_x363_y683_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col9_x400_y683_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col11_x439_y677_w8_h19.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col12_x466_y687_w11_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col13_x494_y683_w16_h12.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row31_col1_x107_y710_w12_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col2_x144_y711_w10_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col3_x180_y711_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col4_x220_y711_w12_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col5_x254_y712_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col6_x289_y712_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col7_x326_y711_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col8_x364_y711_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col9_x399_y707_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col10_x428_y712_w19_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col12_x466_y712_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col13_x497_y713_w10_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Adaptive thresholding on grayscale image\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 5)\n",
    "    return thresh\n",
    "\n",
    "# Function to save segmented parts\n",
    "def save_segment(segment, subgroup_range, index, part_type):\n",
    "    \"\"\"\n",
    "    Function to save a segmented part and return its path.\n",
    "    \n",
    "    Parameters:\n",
    "    - segment: The segmented image.\n",
    "    - subgroup_range: The subgroup range.\n",
    "    - index: The index in the list.\n",
    "    - part_type: Type of segment ('left', 'mid', 'right').\n",
    "    \n",
    "    Returns:\n",
    "    - Path to the saved segment.\n",
    "    \"\"\"\n",
    "    output_folder = os.path.normpath('Analysis/yaman_1_taal_segmented')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    segment_filename = f\"{subgroup_range[0]}_{subgroup_range[1]}_{index}_{part_type}.png\"\n",
    "    segment_path = os.path.join(output_folder, segment_filename)\n",
    "    cv2.imwrite(segment_path, segment)\n",
    "    \n",
    "    return segment_path\n",
    "\n",
    "# Function to extract meend and kann swar segments\n",
    "def extract_alphabets_vertical(image_path):\n",
    "    \"\"\"\n",
    "    Function to perform vertical segmentation on an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_path: Path to the image file.\n",
    "    \n",
    "    Returns:\n",
    "    - left_part: Left part of the image (kann swar or None).\n",
    "    - mid_part: Mid part of the image (meend).\n",
    "    - right_part: Right part of the image (kann swar or None).\n",
    "    \"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None, None, None\n",
    "\n",
    "    processed_image = preprocess_image(image)\n",
    "    contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    valid_coords = []\n",
    "    all_coords = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if 10 <= h <= 25 and w > 25:\n",
    "            valid_coords.append((x, y, w, h))\n",
    "        else:\n",
    "            all_coords.append((x, y, w, h))\n",
    "\n",
    "    left_part, mid_part, right_part = None, None, None\n",
    "    \n",
    "    if valid_coords:\n",
    "        valid_coords = sorted(valid_coords, key=lambda coord: coord[0])\n",
    "        leftmost_valid = valid_coords[0]\n",
    "        rightmost_valid = valid_coords[-1]\n",
    "\n",
    "        left_cut = None\n",
    "        x1, y1, w1, h1 = leftmost_valid\n",
    "        for x, y, w, h in all_coords:\n",
    "            if x <= x1 and (y + h) >= y1 and h > 10 and w > 10:\n",
    "                left_cut = x1\n",
    "                break\n",
    "\n",
    "        right_cut = None\n",
    "        x2, y2, w2, h2 = rightmost_valid\n",
    "        for x, y, w, h in all_coords:\n",
    "            if x >= (x2 + w2) and (y + h) >= y2 and h > 10 and w > 10:\n",
    "                right_cut = x2 + w2\n",
    "                break\n",
    "\n",
    "        if left_cut is not None and right_cut is not None:\n",
    "            left_part = image[:, :left_cut]\n",
    "            mid_part = image[:, left_cut:right_cut]\n",
    "            right_part = image[:, right_cut:]\n",
    "\n",
    "        elif left_cut is not None:\n",
    "            left_part = image[:, :left_cut]\n",
    "            right_part = image[:, left_cut:]\n",
    "        \n",
    "        elif right_cut is not None:\n",
    "            left_part = image[:, :right_cut]\n",
    "            right_part = image[:, right_cut:]\n",
    "\n",
    "    # If no segmentation occurred, treat the entire image as mid_part\n",
    "    if left_part is None and right_part is None:\n",
    "        mid_part = image\n",
    "\n",
    "    return left_part, mid_part, right_part\n",
    "\n",
    "# Function to identify meend and kann swar\n",
    "def identify_meend_and_kann_swar(left_part, mid_part, right_part):\n",
    "    \"\"\"\n",
    "    Function to identify and structure meend and kann swar based on the width of the segments.\n",
    "    \n",
    "    Parameters:\n",
    "    - left_part: The left part of the image.\n",
    "    - mid_part: The mid part of the image.\n",
    "    - right_part: The right part of the image.\n",
    "    \n",
    "    Returns:\n",
    "    - left_part: The left part (kann swar or None).\n",
    "    - mid_part: The mid part (meend).\n",
    "    - right_part: The right part (kann swar or None).\n",
    "    \"\"\"\n",
    "    # Determine which part is meend based on width\n",
    "    parts = {\n",
    "        \"left\": left_part,\n",
    "        \"mid\": mid_part,\n",
    "        \"right\": right_part\n",
    "    }\n",
    "\n",
    "    # Filter out None parts\n",
    "    valid_parts = {k: v for k, v in parts.items() if v is not None}\n",
    "\n",
    "    # If no segmentation occurred (only mid_part exists)\n",
    "    if len(valid_parts) == 1 and \"mid\" in valid_parts:\n",
    "        # Treat the entire image as meend\n",
    "        left_part = None\n",
    "        right_part = None\n",
    "        mid_part = valid_parts[\"mid\"]\n",
    "    \n",
    "    # If there are only two parts, identify meend based on width\n",
    "    elif len(valid_parts) == 2:\n",
    "        # Find the part with the maximum width (meend)\n",
    "        meend_key = max(valid_parts, key=lambda k: valid_parts[k].shape[1])\n",
    "        kann_swar_key = [k for k in valid_parts.keys() if k != meend_key][0]\n",
    "\n",
    "        # Reassign parts to ensure meend is in the middle\n",
    "        if meend_key == \"left\":\n",
    "            mid_part = valid_parts[meend_key]\n",
    "            right_part = valid_parts[kann_swar_key]\n",
    "            left_part = None\n",
    "        elif meend_key == \"right\":\n",
    "            mid_part = valid_parts[meend_key]\n",
    "            left_part = valid_parts[kann_swar_key]\n",
    "            right_part = None\n",
    "        else:\n",
    "            # If meend is already in the middle, no changes needed\n",
    "            pass\n",
    "\n",
    "    # If there are three parts, meend is always in the middle\n",
    "    elif len(valid_parts) == 3:\n",
    "        mid_part = valid_parts[\"mid\"]\n",
    "        left_part = valid_parts[\"left\"]\n",
    "        right_part = valid_parts[\"right\"]\n",
    "\n",
    "    return left_part, mid_part, right_part\n",
    "\n",
    "# Function to update kann swar and meend lists\n",
    "def update_kann_swar_and_meend_lists(subgroup_results):\n",
    "    \"\"\"\n",
    "    Function to update kann swar and meend lists based on segmentation.\n",
    "    \n",
    "    Parameters:\n",
    "    - subgroup_results: Dictionary containing subgroup results.\n",
    "    \"\"\"\n",
    "    for subgroup_range, results in subgroup_results.items():\n",
    "        kann_swar_list = results['kann_swar_list']\n",
    "        swar_list = results['swar_list']\n",
    "        \n",
    "        # Initialize meend list with empty values\n",
    "        meend_list = ['' for _ in range(len(swar_list))]\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(kann_swar_list):\n",
    "            if kann_swar_list[i]:  # Check if the list is not empty\n",
    "                image_path = kann_swar_list[i][0]\n",
    "                # Extract width from the filename\n",
    "                filename = os.path.basename(image_path)\n",
    "                width = int(filename.split('_w')[1].split('_')[0])\n",
    "                \n",
    "                if width > 20:  # Only process if width > 20\n",
    "                    # Perform segmentation\n",
    "                    left_part, mid_part, right_part = extract_alphabets_vertical(image_path)\n",
    "                    \n",
    "                    # Identify and structure meend and kann swar\n",
    "                    left_part, mid_part, right_part = identify_meend_and_kann_swar(left_part, mid_part, right_part)\n",
    "                    \n",
    "                    if mid_part is not None:  # If meend is found\n",
    "                        # Mark start of meend\n",
    "                        meend_list[i] = 'S'\n",
    "                        \n",
    "                        # Calculate x + w for the current image\n",
    "                        x = int(filename.split('_x')[1].split('_')[0])\n",
    "                        w = width\n",
    "                        x_end = x + w\n",
    "                        \n",
    "                        # Find the end of meend\n",
    "                        j = i + 1\n",
    "                        while j < len(swar_list):\n",
    "                            swar_image_path = swar_list[j][0]\n",
    "                            swar_filename = os.path.basename(swar_image_path)\n",
    "                            swar_x = int(swar_filename.split('_x')[1].split('_')[0])\n",
    "                            \n",
    "                            if swar_x >= x_end:\n",
    "                                break  # Stop if swar_x is outside meend area\n",
    "                            j += 1\n",
    "                        \n",
    "                        # Mark end of meend\n",
    "                        if j > i:\n",
    "                            meend_list[j - 1] = 'E'\n",
    "                        \n",
    "                        # Update kann swar list based on segmentation\n",
    "                        if left_part is not None:\n",
    "                            kann_swar_list[i] = [save_segment(left_part, subgroup_range, i, 'left')]\n",
    "                        if right_part is not None:\n",
    "                            kann_swar_list[j - 1] = [save_segment(right_part, subgroup_range, j - 1, 'right')]\n",
    "                        if left_part is None and right_part is None:\n",
    "                            kann_swar_list[i] = []  # Remove the original image if no segmentation\n",
    "                        \n",
    "                        # Skip processed indices\n",
    "                        i = j\n",
    "                    else:\n",
    "                        i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Update the subgroup results with the meend list\n",
    "        subgroup_results[subgroup_range]['meend_list'] = meend_list\n",
    "\n",
    "# Example usage\n",
    "update_kann_swar_and_meend_lists(subgroup_results)\n",
    "\n",
    "# Print the updated results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(f\"Meend List: {results['meend_list']}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Lyrics List: {results['lyrics_list']}\")\n",
    "    print(f\"Swar Articulation Checks: {results['swar_articulation_checks']}\")\n",
    "    print(f\"Lyrics Articulation Checks: {results['lyrics_articulation_checks']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 6)\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row4_col1_x102_y134_w8_h10.png'], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row4_col4_x212_y134_w11_h9.png'], ['Analysis\\\\yaman_1_taal\\\\0_row4_col5_x249_y133_w10_h10.png'], [], ['Analysis\\\\yaman_1_taal\\\\0_row4_col7_x322_y130_w9_h12.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row5_col1_x104_y147_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col2_x145_y147_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col3_x177_y146_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col4_x216_y147_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col5_x254_y143_w10_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col6_x295_y149_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col7_x327_y150_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col8_x364_y150_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col9_x400_y144_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col10_x436_y150_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col11_x467_y154_w8_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row5_col12_x501_y149_w10_h13.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row6_col1_x105_y180_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col2_x146_y180_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col3_x177_y180_w11_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col4_x217_y175_w15_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col5_x251_y180_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col6_x295_y180_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col7_x327_y179_w8_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col8_x363_y179_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col9_x399_y179_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col10_x434_y179_w16_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col11_x466_y179_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row6_col12_x500_y180_w9_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (8, 9)\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row8_col1_x103_y213_w10_h12.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal_segmented\\\\0_row8_col7_x321_y216_w13_h9_upper.png'], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row8_col10_x434_y212_w7_h13.png'], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row8_col1_x104_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col2_x148_y237_w9_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col3_x178_y233_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col4_x221_y227_w9_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col5_x250_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col6_x295_y232_w9_h13.png'], ['Analysis\\\\yaman_1_taal_segmented\\\\0_row8_col7_x321_y228_w13_h15_lower.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col8_x362_y233_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col9_x400_y227_w7_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col10_x431_y229_w15_h23.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col11_x465_y227_w7_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row8_col12_x496_y233_w16_h12.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row9_col1_x104_y256_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col2_x151_y261_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col3_x177_y261_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col4_x224_y262_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col5_x250_y262_w11_h15.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col6_x295_y261_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col7_x326_y262_w12_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col8_x365_y261_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col9_x400_y262_w9_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col10_x434_y262_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col11_x466_y261_w7_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row9_col12_x499_y262_w11_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (11, 11)\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row11_col1_x104_y306_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col2_x143_y307_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col3_x176_y301_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col4_x223_y301_w9_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col5_x250_y307_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col6_x294_y307_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col7_x327_y301_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col8_x364_y301_w9_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col9_x400_y307_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col10_x437_y307_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col11_x465_y307_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row11_col12_x500_y307_w11_h12.png']]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (13, 14)\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Kann Swar List: [[], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row13_col7_x322_y339_w13_h14.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row14_col1_x105_y356_w14_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col2_x143_y356_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col3_x177_y355_w8_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col4_x223_y357_w8_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col5_x250_y356_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col6_x297_y355_w8_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col7_x328_y356_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col8_x370_y355_w7_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col9_x399_y356_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col10_x431_y356_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col11_x465_y360_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row14_col12_x501_y360_w10_h12.png']]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (17, 19)\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row17_col1_x103_y414_w10_h12.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row17_col7_x322_y418_w7_h10.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row18_col1_x108_y434_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col2_x145_y434_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col3_x181_y435_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col4_x218_y439_w11_h5.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col5_x255_y434_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col6_x292_y435_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col7_x327_y431_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col8_x366_y440_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col9_x396_y431_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col10_x436_y431_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col11_x466_y440_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row18_col12_x494_y431_w17_h16.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row19_col1_x108_y463_w12_h15.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col2_x147_y463_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col3_x182_y464_w14_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col4_x221_y464_w8_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col5_x255_y460_w15_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col6_x290_y460_w11_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col7_x327_y464_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col8_x367_y464_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col9_x397_y463_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col10_x435_y464_w16_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col11_x466_y464_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row19_col12_x499_y465_w11_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (21, 23)\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', 'S', 'E', '']\n",
      "Kann Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row21_col1_x102_y495_w14_h14.png'], [], [], [], [], [], [], [], [], ['Analysis\\\\yaman_1_taal_segmented\\\\21_23_9_left.png'], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row22_col1_x108_y512_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col2_x138_y512_w17_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col3_x181_y512_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col4_x217_y522_w10_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col5_x254_y513_w10_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col6_x293_y512_w8_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col7_x327_y513_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col8_x357_y514_w17_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col10_x399_y513_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col11_x431_y513_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col12_x465_y518_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row22_col13_x501_y518_w9_h12.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row23_col1_x108_y545_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col2_x144_y546_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col3_x181_y546_w14_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col4_x219_y546_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col5_x253_y541_w14_h20.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col6_x293_y546_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col7_x327_y547_w12_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col9_x366_y547_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col10_x400_y542_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col11_x428_y546_w19_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col12_x466_y547_w8_h11.png'], ['Analysis\\\\yaman_1_taal\\\\0_row23_col13_x501_y547_w9_h14.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (25, 27)\n",
      "Meend List: ['S', 'E', '', '', '', '', '', '', '', '', '', '']\n",
      "Kann Swar List: [[], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row25_col7_x321_y583_w7_h9.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row26_col1_x108_y599_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col2_x143_y599_w11_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col3_x180_y599_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col4_x218_y599_w9_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col5_x252_y599_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col6_x291_y600_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col7_x326_y596_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col8_x358_y596_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col9_x400_y600_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col10_x437_y600_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col11_x466_y600_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row26_col12_x501_y600_w9_h13.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row27_col1_x108_y628_w13_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col2_x146_y628_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col3_x179_y628_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col4_x216_y628_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col5_x253_y629_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col6_x290_y629_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col7_x327_y629_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col8_x363_y630_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col9_x400_y630_w11_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col10_x438_y630_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col11_x467_y629_w7_h14.png'], ['Analysis\\\\yaman_1_taal\\\\0_row27_col12_x503_y629_w8_h13.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 31)\n",
      "Meend List: ['', '', '', '', '', '', '', '', '', '', '', '']\n",
      "Kann Swar List: [[], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row29_col4_x211_y665_w10_h9.png'], [], [], [], [], [], ['Analysis\\\\yaman_1_taal\\\\0_row29_col10_x429_y666_w10_h9.png'], [], []]\n",
      "Swar List: [['Analysis\\\\yaman_1_taal\\\\0_row30_col1_x107_y677_w16_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col2_x138_y678_w16_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col3_x177_y677_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col4_x216_y678_w15_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col5_x254_y677_w11_h18.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col6_x291_y683_w9_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col7_x325_y683_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col8_x363_y683_w11_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col9_x400_y683_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col11_x439_y677_w8_h19.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col12_x466_y687_w11_h4.png'], ['Analysis\\\\yaman_1_taal\\\\0_row30_col13_x494_y683_w16_h12.png']]\n",
      "Lyrics List: [['Analysis\\\\yaman_1_taal\\\\0_row31_col1_x107_y710_w12_h13.png'], ['Analysis\\\\yaman_1_taal_segmented\\\\0_row31_col2_x144_y711_w10_h20_seg1.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col3_x180_y711_w8_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col4_x220_y711_w12_h16.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col5_x254_y712_w10_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col6_x289_y712_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col7_x326_y711_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col8_x364_y711_w10_h13.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col9_x399_y707_w14_h17.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col10_x428_y712_w19_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col12_x466_y712_w8_h12.png'], ['Analysis\\\\yaman_1_taal\\\\0_row31_col13_x497_y713_w10_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import binary_erosion, binary_dilation, square\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Define the path to the folder to store segmented images\n",
    "segmented_folder_path = os.path.normpath('Analysis/yaman_1_taal_segmented')\n",
    "os.makedirs(segmented_folder_path, exist_ok=True)\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 5)\n",
    "    return thresh\n",
    "\n",
    "# Function to separate articulation in an image\n",
    "def separate_articulation(image):\n",
    "    processed_image = preprocess_image(image)\n",
    "    contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if 10 < h < 21 and w > 25:\n",
    "            upper_part = image[:y, :]\n",
    "            if upper_part.shape[0] > 0:\n",
    "                return upper_part, True  # Return the upper part and a flag indicating segmentation was successful\n",
    "            break\n",
    "    \n",
    "    return image, False  # Return the original image and a flag indicating no segmentation\n",
    "\n",
    "# Function to segment a word into multiple images\n",
    "def segment_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply simple binary thresholding and invert the image\n",
    "    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Define structuring elements\n",
    "    structuring_element2 = np.ones((2, 2), dtype=bool)\n",
    "    structuring_element_erosion = square(3)\n",
    "\n",
    "    # Apply binary dilation to fill gaps\n",
    "    dilated = binary_dilation(binary, footprint=structuring_element2)\n",
    "\n",
    "    # Apply binary erosion to separate connected components\n",
    "    eroded = binary_erosion(dilated, footprint=structuring_element_erosion)\n",
    "    eroded = img_as_ubyte(eroded)  # Convert to uint8 for display purposes\n",
    "\n",
    "    # Perform vertical projection to find potential cut lines\n",
    "    vertical_projection = np.sum(eroded, axis=0)\n",
    "\n",
    "    # Find cut points by identifying valleys in the projection with heuristic\n",
    "    threshold = 0.15 * np.max(vertical_projection)\n",
    "    valleys = [x for x, y in enumerate(vertical_projection) if y < threshold]\n",
    "\n",
    "    # Apply heuristic: if two consecutive valleys are close, take the right one\n",
    "    cut_points = []\n",
    "    min_distance = 13\n",
    "    i = 0\n",
    "    while i < len(valleys) - 1:\n",
    "        if (valleys[i + 1] - valleys[i]) < min_distance:\n",
    "            cut_points.append(valleys[i + 1])\n",
    "            i += 2  # Skip the next valley since we took the right one\n",
    "        else:\n",
    "            cut_points.append(valleys[i])\n",
    "            i += 1\n",
    "    if i == len(valleys) - 1:\n",
    "        cut_points.append(valleys[i])  # Add the last valley if it's not processed\n",
    "\n",
    "    # Ensure no duplicate cut points and sort them\n",
    "    cut_points = sorted(set(cut_points))\n",
    "\n",
    "    # Separate the image at cut points\n",
    "    cut_images = []\n",
    "    start = 0\n",
    "    for cut_point in cut_points:\n",
    "        if cut_point - start > 10:  # Ensure segments are large enough\n",
    "            cut_image = img[:, start:cut_point]\n",
    "            cut_images.append(cut_image)\n",
    "            start = cut_point\n",
    "\n",
    "    # Add the last segment\n",
    "    cut_images.append(img[:, start:])\n",
    "\n",
    "    return cut_images\n",
    "\n",
    "# Function to merge segments based on height-to-width ratio\n",
    "def merge_segments(segments):\n",
    "    final_images = []\n",
    "    i = 0\n",
    "    while i < len(segments):\n",
    "        current_image = segments[i]\n",
    "        current_ratio = current_image.shape[0] / current_image.shape[1]\n",
    "\n",
    "        ratio_threshold = 1.8\n",
    "\n",
    "        if current_image.shape[0] > 35:\n",
    "            ratio_threshold = 2.9\n",
    "        \n",
    "        # If the ratio is greater than the threshold and it's the first segment\n",
    "        if current_ratio > ratio_threshold and i == 0:\n",
    "            # Merge with the next segment\n",
    "            if i + 1 < len(segments):\n",
    "                current_image = np.hstack((current_image, segments[i + 1]))\n",
    "                final_images.append(current_image)\n",
    "                i += 2\n",
    "            else:\n",
    "                final_images.append(current_image)\n",
    "                i += 1\n",
    "        # If two or more consecutive segments have a ratio greater than the threshold\n",
    "        elif i < len(segments) - 1 and (segments[i + 1].shape[0] / segments[i + 1].shape[1]) > ratio_threshold:\n",
    "            while i < len(segments) - 1 and (segments[i + 1].shape[0] / segments[i + 1].shape[1]) > ratio_threshold:\n",
    "                current_image = np.hstack((current_image, segments[i + 1]))\n",
    "                i += 1\n",
    "            final_images.append(current_image)\n",
    "            i += 1\n",
    "        # If the ratio is greater than the threshold and it's not the first segment\n",
    "        elif current_ratio > ratio_threshold and i != 0:\n",
    "            # Merge with the previous segment\n",
    "            if final_images:\n",
    "                final_images[-1] = np.hstack((final_images[-1], current_image))\n",
    "            else:\n",
    "                final_images.append(current_image)\n",
    "            i += 1\n",
    "        else:\n",
    "            final_images.append(current_image)\n",
    "            i += 1\n",
    "\n",
    "    return final_images\n",
    "\n",
    "# Function to process a single image, segment, and save the results in the provided folder\n",
    "def segment_word(image_path, output_folder):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return []\n",
    "    \n",
    "    # Segment the image\n",
    "    segmented_images = segment_image(img)\n",
    "    \n",
    "    # Merge segments based on height-to-width ratio\n",
    "    final_images = merge_segments(segmented_images)\n",
    "    \n",
    "    # Save the segmented images\n",
    "    image_base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    segmented_paths = []\n",
    "    for i, segmented_image in enumerate(final_images):\n",
    "        seg_image_path = os.path.normpath(os.path.join(output_folder, f'{image_base_name}_seg{i+1}.png'))\n",
    "        cv2.imwrite(seg_image_path, segmented_image)\n",
    "        segmented_paths.append(seg_image_path)\n",
    "    \n",
    "    return segmented_paths\n",
    "\n",
    "# Function to update lists based on segmentation\n",
    "def update_lists_with_segmentation(subgroup_results):\n",
    "    for subgroup_range, results in subgroup_results.items():\n",
    "        swar_list = results['swar_list']\n",
    "        lyrics_list = results['lyrics_list']\n",
    "        swar_articulation_checks = results['swar_articulation_checks']\n",
    "        lyrics_articulation_checks = results['lyrics_articulation_checks']\n",
    "        \n",
    "        # Apply articulation segmentation to swar row\n",
    "        for i in range(len(swar_list)):\n",
    "            if not swar_articulation_checks[i] and swar_list[i]:  # Check if articulation is False and the list is not empty\n",
    "                swar_image_path = swar_list[i][0]  # Get the image path\n",
    "                swar_image = cv2.imread(swar_image_path)  # Load the image\n",
    "                if swar_image is not None:\n",
    "                    segmented_image, is_segmented = separate_articulation(swar_image)\n",
    "                    if is_segmented:\n",
    "                        swar_articulation_checks[i] = True  # Update articulation check\n",
    "                        # Save the segmented image with the original name\n",
    "                        original_name = os.path.basename(swar_image_path)\n",
    "                        seg_image_path = os.path.normpath(os.path.join(segmented_folder_path, original_name))\n",
    "                        cv2.imwrite(seg_image_path, segmented_image)\n",
    "                        swar_list[i] = [seg_image_path]  # Update the list with the new image path\n",
    "        \n",
    "        # Apply articulation segmentation to lyrics row\n",
    "        for i in range(len(lyrics_list)):\n",
    "            if not lyrics_articulation_checks[i] and lyrics_list[i]:  # Check if articulation is False and the list is not empty\n",
    "                lyrics_image_path = lyrics_list[i][0]  # Get the image path\n",
    "                lyrics_image = cv2.imread(lyrics_image_path)  # Load the image\n",
    "                if lyrics_image is not None:\n",
    "                    segmented_image, is_segmented = separate_articulation(lyrics_image)\n",
    "                    if is_segmented:\n",
    "                        lyrics_articulation_checks[i] = True  # Update articulation check\n",
    "                        # Save the segmented image with the original name\n",
    "                        original_name = os.path.basename(lyrics_image_path)\n",
    "                        seg_image_path = os.path.normpath(os.path.join(segmented_folder_path, original_name))\n",
    "                        cv2.imwrite(seg_image_path, segmented_image)\n",
    "                        lyrics_list[i] = [seg_image_path]  # Update the list with the new image path\n",
    "        \n",
    "        # Apply word segmentation to swar row\n",
    "        for i in range(len(swar_list)):\n",
    "            if swar_articulation_checks[i] and swar_list[i]:  # Check if articulation is True and the list is not empty\n",
    "                swar_image_path = swar_list[i][0]  # Get the image path\n",
    "                segmented_paths = segment_word(swar_image_path, segmented_folder_path)\n",
    "                if segmented_paths:\n",
    "                    swar_list[i] = segmented_paths  # Update the list with segmented image paths\n",
    "        \n",
    "        # Apply word segmentation to lyrics row\n",
    "        for i in range(len(lyrics_list)):\n",
    "            if lyrics_articulation_checks[i] and lyrics_list[i]:  # Check if articulation is True and the list is not empty\n",
    "                lyrics_image_path = lyrics_list[i][0]  # Get the image path\n",
    "                segmented_paths = segment_word(lyrics_image_path, segmented_folder_path)\n",
    "                if segmented_paths:\n",
    "                    lyrics_list[i] = segmented_paths  # Update the list with segmented image paths\n",
    "        \n",
    "        # Update the results\n",
    "        subgroup_results[subgroup_range]['swar_list'] = swar_list\n",
    "        subgroup_results[subgroup_range]['lyrics_list'] = lyrics_list\n",
    "        subgroup_results[subgroup_range]['swar_articulation_checks'] = swar_articulation_checks\n",
    "        subgroup_results[subgroup_range]['lyrics_articulation_checks'] = lyrics_articulation_checks\n",
    "\n",
    "# Example usage\n",
    "update_lists_with_segmentation(subgroup_results)\n",
    "\n",
    "# Print the updated results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Meend List: {results['meend_list']}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Lyrics List: {results['lyrics_list']}\")\n",
    "    print(f\"Swar Articulation Checks: {results['swar_articulation_checks']}\")\n",
    "    print(f\"Lyrics Articulation Checks: {results['lyrics_articulation_checks']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 557ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Subgroup Range: (4, 6)\n",
      "Predicted Swar List: [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['-'], ['']]\n",
      "Predicted Kann Swar List: [[''], [], [], [''], [''], [], [''], [], [], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (8, 9)\n",
      "Predicted Swar List: [[''], ['-'], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
      "Predicted Kann Swar List: [[''], [], [], [], [], [], [''], [], [], [''], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (11, 11)\n",
      "Predicted Swar List: [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
      "Predicted Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (13, 14)\n",
      "Predicted Swar List: [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
      "Predicted Kann Swar List: [[], [], [], [], [], [], [''], [], [], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (17, 19)\n",
      "Predicted Swar List: [[''], [''], [''], ['-'], [''], [''], [''], ['-'], [''], [''], ['-'], ['']]\n",
      "Predicted Kann Swar List: [[''], [], [], [], [], [], [''], [], [], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (21, 23)\n",
      "Predicted Swar List: [[''], [''], [''], ['-'], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
      "Predicted Kann Swar List: [[''], [], [], [], [], [], [], [], [], [''], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (25, 27)\n",
      "Predicted Swar List: [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]\n",
      "Predicted Kann Swar List: [[], [], [], [], [], [], [''], [], [], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 31)\n",
      "Predicted Swar List: [[''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['-'], ['']]\n",
      "Predicted Kann Swar List: [[], [], [], [''], [], [], [], [], [], [''], [], []]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('cnn_recognizer_music_15_v1.h5')\n",
    "\n",
    "# Preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Unable to read image at path: {image_path}\")\n",
    "    image = cv2.resize(image, (32, 32))  # Resize to match the model's input size\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Pass the image through the model and get predictions\n",
    "def predict_class(image_path):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)\n",
    "    max_probability = np.max(predictions, axis=1)\n",
    "    return predicted_class_index[0], max_probability[0]\n",
    "\n",
    "# Define the classes\n",
    "classes = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \n",
    "           \"\\u0951\", \"\\u0951\", \"\\u0951\", \"\\u0951\", \"\\u0951\", \"\\u0951\", \"\\u0951\", \"\\u0951\", \"'\", \n",
    "           \"\\u093C\", \"\\u093C\", \"\\u093C\", \"\\u093C\", \"\\u093C\", \"\\u093C\", \"\\u093C\", \n",
    "           \")\", \",\", \"-\", \"\", \"O\", \"(\", \"^^\", \"X\", \"\", \"\", \"|\", \"<_>\"]\n",
    "\n",
    "# Function to generate new lists with predicted class names\n",
    "def generate_predicted_lists(subgroup_results):\n",
    "    predicted_results = {}\n",
    "    \n",
    "    for subgroup_range, results in subgroup_results.items():\n",
    "        # Initialize new lists for predicted class names\n",
    "        predicted_swar_list = []\n",
    "        predicted_kann_swar_list = []  # Add this if you have kann_swar_list\n",
    "        \n",
    "        # Predict class names for swar_list\n",
    "        for image_paths in results['swar_list']:\n",
    "            if image_paths:  # Check if the list is not empty\n",
    "                predicted_classes = []\n",
    "                for image_path in image_paths:\n",
    "                    predicted_class_index, _ = predict_class(image_path)\n",
    "                    predicted_class_name = classes[predicted_class_index]\n",
    "                    predicted_classes.append(predicted_class_name)\n",
    "                predicted_swar_list.append(predicted_classes)\n",
    "            else:\n",
    "                predicted_swar_list.append([])  # Append empty list for empty entries\n",
    "        \n",
    "        # Predict class names for kann_swar_list (if applicable)\n",
    "        for image_paths in results['kann_swar_list']:\n",
    "            if image_paths:  # Check if the list is not empty\n",
    "                predicted_classes = []\n",
    "                for image_path in image_paths:\n",
    "                    predicted_class_index, _ = predict_class(image_path)\n",
    "                    predicted_class_name = classes[predicted_class_index]\n",
    "                    predicted_classes.append(predicted_class_name)\n",
    "                predicted_kann_swar_list.append(predicted_classes)\n",
    "            else:\n",
    "                predicted_kann_swar_list.append([])  # Append empty list for empty entries\n",
    "        \n",
    "        # Store the predicted results for this subgroup\n",
    "        predicted_results[subgroup_range] = {\n",
    "            'predicted_swar_list': predicted_swar_list,\n",
    "            'predicted_kann_swar_list': predicted_kann_swar_list  \n",
    "        }\n",
    "    \n",
    "    return predicted_results\n",
    "\n",
    "# Example usage\n",
    "# Assuming subgroup_results is the dictionary you provided\n",
    "predicted_results = generate_predicted_lists(subgroup_results)\n",
    "\n",
    "# Print the predicted results\n",
    "for subgroup_range, results in predicted_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Predicted Swar List: {results['predicted_swar_list']}\")\n",
    "    print(f\"Predicted Kann Swar List: {results['predicted_kann_swar_list']}\") \n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
